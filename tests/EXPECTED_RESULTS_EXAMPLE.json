{
  "timestamp": "2025-01-15T10:30:00",
  "model": "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ",
  "tests": [
    {
      "test_name": "multi_hop_reasoning",
      "retrieval": {
        "response": "Based on our discussion, here's a 5-day itinerary for your Japan trip:\n\nDay 1-2: Tokyo (modern attractions, student-friendly hostels, $80-100/day)\nDay 3-5: Kyoto (traditional architecture: Kinkaku-ji, Fushimi Inari temples)\n\nBring your student ID for discounts at temples and museums. Pack layers and a light jacket for March weather (10-15°C).",
        "retrieval_latency_ms": 23.4,
        "generation_latency_ms": 892.1,
        "prompt_tokens": 342,
        "response_tokens": 156,
        "total_tokens": 498,
        "retrieved_context_count": 3,
        "method": "retrieval"
      },
      "full_context": {
        "response": "Based on everything we've discussed, here's a comprehensive 5-day itinerary:\n\nDay 1-2: Tokyo - Explore modern attractions. Stay in budget hostels ($80-100/day as student)\nDay 3-5: Kyoto - Focus on traditional architecture (Kinkaku-ji, Fushimi Inari). 3-4 days recommended for temple visits.\n\nWeather: March is mild (10-15°C) - pack layers, light jacket, walking shoes.\nBudget: $80-100/day includes accommodation, food, local transport.\nTip: Bring student ID for temple/museum discounts!",
        "retrieval_latency_ms": 0,
        "generation_latency_ms": 1124.3,
        "prompt_tokens": 1287,
        "response_tokens": 162,
        "total_tokens": 1449,
        "retrieved_context_count": 0,
        "method": "full_context"
      },
      "text_similarity": 0.87,
      "semantic_similarity": 0.92,
      "token_savings_pct": 73.4
    },
    {
      "test_name": "context_dependent_queries",
      "retrieval": {
        "response": "You chose JWT tokens with Supabase for authentication. This works perfectly with your database choice because Supabase already provides PostgreSQL, and JWT verification is built-in. You can validate tokens in middleware and inject the user into request.state, giving you a complete auth solution.",
        "retrieval_latency_ms": 18.2,
        "generation_latency_ms": 743.5,
        "prompt_tokens": 298,
        "response_tokens": 89,
        "total_tokens": 387,
        "retrieved_context_count": 3,
        "method": "retrieval"
      },
      "full_context": {
        "response": "You selected JWT tokens with Supabase for authentication. This choice integrates seamlessly with your database decision because Supabase comes with PostgreSQL built-in. The JWT verification is handled by Supabase's SDK, and you can implement middleware to validate tokens and store user data in request.state for easy access throughout your FastAPI app.",
        "retrieval_latency_ms": 0,
        "generation_latency_ms": 891.2,
        "prompt_tokens": 1145,
        "response_tokens": 95,
        "total_tokens": 1240,
        "retrieved_context_count": 0,
        "method": "full_context"
      },
      "text_similarity": 0.91,
      "semantic_similarity": 0.95,
      "token_savings_pct": 74.0
    },
    {
      "test_name": "code_understanding",
      "retrieval": {
        "response": "Here's a summary of the debugging steps:\n\n1. Initial error: 'NoneType' in llm_proxy_service.py line 47 - caused by provider_config being None\n2. Root cause: Model 'gpt-4-custom' not in registry\n3. Fix 1: Added model to provider_config.py OpenAI models list\n4. Second error: 'API key not found'\n5. Fix 2: Checked api_key_service.py decrypt_api_key() - verified API_KEY_ENCRYPTION_KEY env var\n\nFiles modified: llm_proxy_service.py, provider_config.py, api_key_service.py",
        "retrieval_latency_ms": 21.7,
        "generation_latency_ms": 856.3,
        "prompt_tokens": 356,
        "response_tokens": 142,
        "total_tokens": 498,
        "retrieved_context_count": 3,
        "method": "retrieval"
      },
      "full_context": {
        "response": "Debugging summary for /chat endpoint:\n\n1. Error: 'NoneType object has no attribute get' in llm_proxy_service.py:47\n   - Cause: detect_provider() returned None for 'gpt-4-custom'\n   - Fix: Added 'gpt-4-custom' to OpenAI models in provider_config.py\n\n2. Error: 'API key not found' after fix 1\n   - Investigated: Key was encrypted in database\n   - Fix: Verified API_KEY_ENCRYPTION_KEY env var in api_key_service.py decrypt_api_key()\n   - Ensured same Fernet key used for encryption/decryption\n\nFiles touched: llm_proxy_service.py, provider_config.py, api_key_service.py",
        "retrieval_latency_ms": 0,
        "generation_latency_ms": 1087.9,
        "prompt_tokens": 1423,
        "response_tokens": 167,
        "total_tokens": 1590,
        "retrieved_context_count": 0,
        "method": "full_context"
      },
      "text_similarity": 0.89,
      "semantic_similarity": 0.93,
      "token_savings_pct": 75.0
    },
    {
      "test_name": "long_form_generation",
      "retrieval": {
        "response": "Stop paying for massive context windows. Traditional LLM APIs charge per token, meaning costs explode as conversations grow longer. At turn 100, you're paying for 10,000+ tokens every single request.\n\nSemantic memory solves this. Instead of sending entire conversation history, retrieve only relevant context. The result? 99% cost reduction while maintaining quality. You get constant token usage per request, infinite conversation length, and predictable costs.\n\nThis approach is perfect for customer support teams, personal AI assistants, and long-running projects where conversations span weeks or months. Complex reasoning and creative writing may need more context, but for most real-world use cases, semantic retrieval delivers the perfect balance of quality and cost savings.",
        "retrieval_latency_ms": 26.8,
        "generation_latency_ms": 1124.7,
        "prompt_tokens": 387,
        "response_tokens": 201,
        "total_tokens": 588,
        "retrieved_context_count": 3,
        "method": "retrieval"
      },
      "full_context": {
        "response": "Stop paying for massive context windows. Here's how semantic memory reduces LLM costs by 99%.\n\nTraditional LLM APIs force you to send entire conversation history with every request. As conversations grow, so do your costs. By turn 100, you're burning through 10,000+ tokens per request, hitting context limits and paying exponentially more.\n\nSemantic memory changes the game. Instead of sending everything, intelligent retrieval pulls only relevant context from past exchanges. Benefits include constant token usage per request, infinite conversation length, and 99% cost reduction. The tradeoffs? Slight quality degradation for complex reasoning, 20-50ms retrieval latency, and need for a vector database.\n\nWho benefits most? Customer support teams, personal assistant apps, and long-running collaborative projects see massive wins. Creative writing and complex analysis may need traditional full-context approaches. Implementation requires Qdrant, embeddings models, hybrid search combining dense and sparse retrieval, and sliding window logic—moderate complexity but worth it for teams hitting context limits or cost constraints.",
        "retrieval_latency_ms": 0,
        "generation_latency_ms": 1456.2,
        "prompt_tokens": 1501,
        "response_tokens": 237,
        "total_tokens": 1738,
        "retrieved_context_count": 0,
        "method": "full_context"
      },
      "text_similarity": 0.85,
      "semantic_similarity": 0.90,
      "token_savings_pct": 74.2
    }
  ],
  "aggregate": {
    "avg_text_similarity": 0.88,
    "avg_semantic_similarity": 0.925,
    "avg_token_savings_pct": 74.1,
    "total_duration_seconds": 47.3
  },
  "interpretation": {
    "quality_score": "EXCELLENT (≥90%)",
    "verdict": "Production ready! Retrieval maintains 92.5% semantic similarity vs full context.",
    "token_savings": "Average 74.1% reduction in prompt tokens",
    "recommendation": "Ship with confidence. Consider 'safe' mode for complex reasoning, 'balanced' for most use cases."
  }
}
